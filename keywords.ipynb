{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd84e34b49eb8105",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# hide UserWarning\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.get_paper import get_papers_to_keyword\n",
    "from utils.ai_tools import add_keywords\n",
    "from utils.data_tools import flatten\n",
    "\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm, trange\n",
    "tqdm.pandas()\n",
    "\n",
    "keyword = 'Pervasive Computing' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb4c123e467342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_searched = set()\n",
    "all_papers = pd.DataFrame()\n",
    "all_papers = pd.concat([all_papers, add_keywords(get_papers_to_keyword(keyword = keyword, logging=True),logging=True)])\n",
    "new_keywords = flatten(all_papers['keywords'].to_list())\n",
    "keywords_searched.add(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292197c70aa8e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_levels = 4\n",
    "\n",
    "for i in trange(n_levels):\n",
    "    # Create a pool once outside the loop (consider performance implications)\n",
    "    with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "        # Process keywords in parallel\n",
    "        results = pool.map(get_papers_to_keyword, new_keywords)\n",
    "\n",
    "        # Concatenate results efficiently\n",
    "        all_results = pd.concat(results)\n",
    "\n",
    "    print(f\"Level {i}: {len(all_results)} papers found.\")\n",
    "    # Add keywords\n",
    "    all_results = all_results.reset_index(drop=True)\n",
    "    all_results = add_keywords(all_results, logging=True)\n",
    "    all_papers = pd.concat([all_papers, all_results])\n",
    "\n",
    "    # Update new_keywords \n",
    "    new_keywords = [kw for kw in set(flatten(all_papers['keywords'].to_list())) if kw not in keywords_searched]\n",
    "    keywords_searched.update(new_keywords)\n",
    "    \n",
    "    # Save the results as a pickle file\n",
    "    meta_data = {'keywords_searched': keywords_searched, 'new_keywords': new_keywords}\n",
    "    with open(f'./data/keywords_{i}_meta.pkl', 'wb') as f:\n",
    "        pickle.dump(meta_data, f)\n",
    "    with open(f'./data/keywords_{i}_papers.pkl', 'wb') as f:\n",
    "        pickle.dump(all_papers, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6d20896ac16f50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
